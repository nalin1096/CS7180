\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{CS 7180 Project Proposal}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Nalin Gupta \\
  \texttt{gupta.nal@husky.neu.edu} \\
  \And
  Christopher Botica\\
  \texttt{botica.c@husky.neu.edu} \\
  \And
  Tyler Brown\\
  \texttt{brown.tyler@husky.neu.edu} \\
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Introduction}

Imaging in low-light conditions is challenging due to low-photon count
and low Signal-to-Noise (SNR) ratio. These yield very dark and noisy images.
The most common technique to overcome this problem is long exposure shot.
However, this method yields blurry images with the slightest camera shake
or object motion. Common post-processing techniques brighten the image at
the expense of image quality. Being able to “see in the dark” provides a
number of real-world benefits such as photography, computer vision, and
social networking. We propose a deep-learning model that processes low-light
images to improve image brightness while retaining quality.


In the past, the problem of enhancing low light images has been tackled via
noise reduction. This noise becomes dominant especially in low-light images
due to low SNR. Remez et. al. proposed a deep CNN for noise reduction under
the assumption that this low-light noise belongs to a Poisson
distribution \cite{remez2017deep}.  They used images from ImageNet as their ground truth data
and added synthetic Poisson noise to simulate corrupted images. Even though
their model outperform the state-of-the art de-noiser “BM3D”, it does not
scale well to real world images, due to their underlying assumptions.
Furthermore, their model only denoises images but does not brighten them.
Motivated by these downfalls, Chen et. al., proposed an end-to-end CNN,
“See-in-the-Dark” (SID), which brightens extremely low light images and
removes noise without making any underlying assumptions
\cite{chen2018learning}. However these advances come with the added expense
of collecting large amounts of low
light and bright light images. In the absence of a true vs noisy image
dataset, the team captured scenes using various exposure times to generate
true (bright light) and corrupted (low light) image pairs called
“See-in-the-Dark Dataset” (SID Dataset \footnote{https://github.com/cchen156/Learning-to-See-in-the-Dark}). Furthermore, their model is camera
specific and not easily generalizable.

\section{Proposed Idea}

We propose a transferable CNN for image brightening and denoising. Instead
of training our model on actual true (bright light) and corrupted
(low light) image pairs, we use images from the ImageNet dataset as our
baseline and synthetically corrupt these by darkening and adding Poisson
noise. We train our CNN on the synthetic data to obtain our initial model
parameters. Then, using these, and a small fraction of the real image pairs
from the SID Dataset, we adopt a transfer learning approach to update our
model parameters. We then use this model to test on our SID Dataset. In
addition, we aim to test various transfer learning approaches, such as the
traditional transfer learning and zero shot learning. 


The novelty of our approach stems from the idea of “more for less”. Our
model drastically reduces the overhead costs of data collection by
synthesizing readily available training data (ImageNet). This is
particularly beneficial in domains where collecting images pairs is
expensive/time consuming. 


We will use the SID Model as our baseline and our performance measure will be achieving a Peak Signal-to-Noise Ratio (PSNR) greater or equal to the baseline.


\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
