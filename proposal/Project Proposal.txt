(1) introduction


Imaging in low-light conditions is challenging due to low-photon count and low Signal-to-Noise (SNR) ratio. These yield very dark and noisy images. The most common technique to overcome this problem is long exposure shot. However, this method yields blurry images with the slightest camera shake or object motion. Common post-processing techniques brighten the image at the expense of image quality. Being able to “see in the dark” provides a number of real-world benefits such as photography, computer vision, and social networking. We propose a deep-learning model that processes low-light images to improve image brightness while retaining quality.


In the past, the problem of enhancing low light images has been tackled via noise reduction. This noise becomes dominant especially in low-light images due to low SNR. Remez et. al. proposed a deep CNN for noise reduction under the assumption that this low-light noise belongs to a Poisson distribution[1].  They used images from ImageNet as their ground truth data and added synthetic Poisson noise to simulate corrupted images. Even though their model outperform the state-of-the art de-noiser “BM3D”, it does not scale well to real world images, due to their underlying assumptions. Furthermore, their model only denoises images but does not brighten them.  Motivated by these downfalls, Chen et. al., proposed an end-to-end CNN, “See-in-the-Dark” (SID), which brightens extremely low light images and removes noise without making any underlying assumptions[2]. However these advances come with the added expense of collecting large amounts of low light and bright light images. In the absence of a true vs noisy image dataset, the team captured scenes using various exposure times to generate true (bright light) and corrupted (low light) image pairs called “See-in-the-Dark Dataset” (SID Dataset). Furthermore, their model is camera specific and not easily generalizable.
 
(2) proposed Idea


We propose a transferable CNN for image brightening and denoising. Instead of training our model on actual true (bright light) and corrupted (low light) image pairs, we use images from the ImageNet dataset as our baseline and synthetically corrupt these by darkening and Poisson noise. We train our CNN on the synthetic data to obtain our initial model parameters. Then, using these, and a small fraction of the real image pairs from the SID Dataset, we adopt a transfer learning approach to update our model parameters. We then use this model to test on our SID Dataset. In addition, we aim to test various transfer learning approaches, such as the traditional transfer learning and zero shot learning. 


The novelty of our approach stems from the idea of “more for less”. Our model drastically reduces the overhead costs of data collection by synthesizing readily available training data (ImageNet). This is particularly beneficial in domains where collecting images pairs is expensive/time consuming. 


We will use the SID Model as our baseline and our performance measure will be achieving a Peak Signal-to-Noise Ratio (PSNR) greater or equal to the baseline.