\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}

\graphicspath{ {imgs/} }

\title{CS 7180 Final Report}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Nalin Gupta \\
  \texttt{NUID: 001497315} \\
  \And
  Christopher Botica\\
  \texttt{NUID: 001726854} \\
  \And
  Tyler Brown\\
  \texttt{NUID: 001684955} \\
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Introduction}

Images taken in low-light conditions are often too dark, noisy, and
distorted to be used in industrial purposes. We propose a deep-learning
model that processes low-light images to improve image brightness and
increase their overall quality. The problem with imaging in low-light
conditions is challenging due to low-photon count and low
Signal-to-Noise (SNR) ratio. These yield very dark and noisy images. The
most common technique to overcome this problem is long exposure shot.
However, this method yields blurry images with the slightest camera shake
or object motion\cite{chen2018learning}. Common post-processing
techniques brighten the image at the expense of image quality. Being able to
``see in the dark'' provides a number of real-world benefits such as
photography, computer vision, and social networking.

\section{Related Work}

In the past, the problem of enhancing low light images has been tackled via
noise reduction. This noise becomes dominant especially in low-light images
due to low SNR. Remez et. al. proposed a deep CNN for noise reduction under
the assumption that this low-light noise belongs to a Poisson
distribution \cite{remez2017deep}.  They used images from ImageNet
\cite{imagenet_cvpr09} as their ground truth data
and added synthetic Poisson noise to simulate corrupted images. Even though
their model outperform the state-of-the art de-noiser ``BM3D'', it does not
scale well to real world images, due to their underlying assumptions.... which ones...................??????????????????????????...
Furthermore, their model only denoises images but does not brighten them.
Motivated by these downfalls, Chen et. al., proposed an end-to-end CNN,
``See-in-the-Dark'' (SID), which brightens extremely low light images and
removes noise without making any underlying assumptions
\cite{chen2018learning}. However these advances come with the added expense
of collecting large amounts of low
light and bright light images. In the absence of a true vs noisy image
dataset, the team captured scenes using various exposure times to generate
true (bright light) and corrupted (low light) image pairs called
``See-in-the-Dark Dataset'' (SID Dataset
\footnote{https://github.com/cchen156/Learning-to-See-in-the-Dark}). Furthermore,
their model is camera specific and not easily generalizable.\newline

We propose a transferable CNN for image brightening and denoising. Instead
of training our model on actual true (bright light) and corrupted
(low light) image pairs, we use images from the publicly available
RAISE raw images dataset \cite{Dang-Nguyen:2015:RRI:2713168.2713194}. We
use the RAISE raw images dataset as our true images and corrupt these by
simulating low-light conditions for use in our model. We train our CNN on
this synthetic
data to obtain our initial model parameters. A small fraction of the real
images paired from the SID dataset is then used in our transfer learning
\cite{Goodfellow-et-al-2016} approach to update our model parameters.
These model parameters are updated for the particular camera used in the
SID dataset. We then use this model to test on the SID Dataset. In
addition, we aim to test various transfer learning approaches, such as the
traditional transfer learning and zero shot learning \cite{larochelle2008, NIPS2009_3650,socher2013zeroshot}. \newline

The novelty of our approach stems from the idea of ``more for less''. Our
model drastically reduces the overhead costs of data collection by
synthesizing readily available training data (RAISE raw images dataset).
This is
particularly beneficial in domains where collecting images pairs is
expensive/time consuming.

\section{Methods}

!!!... Say something like our flowchart: 
Simulate low light images -> then build deep cnn model --> then transfer learn to SID data .........


\subsection{Low Light Simulation}

We plan to use the RAISE raw images dataset which includes 6,000 high
definition images taken by professional photographers.

As illustrated in Figure 1, the traditional pipeline takes a corrupted image, and applies the following sequence of modules: Reduce Black Level, Denoising, White Balance, and Gamma Correction. The Black Level refers to refers to the level of brightness at the darkest parts of the image, and is reduced by subtracting the minimum pixel value. Denoising is reduced using common algorithms such as BM3D. White Balance refers to the color balance in the image (i.e., white should be true white) and is corrected by re-balancing the intensities of each color RGB. Finally, Gamma Correction controls the overall brightness of the image. We synthetically generate corrupted images by applying the reverse of this pipeline. Gamma Distortion: decrease the brightness of the image, White Imbalance: skew the color-space by multiplying each level of RGB by a random weight, Poisson Noise: add Poisson noise to the image, Black Level: add a negative bias to the pixel values (i.e., random black level). 

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.35]{simulation_flowchart}
  \caption{Top: Traditional Pipeline for processing low-light images. Bottom: Our  pipeline to simulate low-light images based on the traditional, only in reverse.}
\end{figure}

We simulate our low light images by successively applying gamma distortion, white imbalance, Poisson noise, and black level to a normal lighted. An example of two of such images is represented in Figure 3. 

For a given bright image, \(B\), a matrix of size \(m\)x\(n\)x3, we want to simulate a low-light image \(D\), or the same. We define gamma distortion as follows:
\[   D = {B}^{1/\gamma}, \] 
where \(\gamma\) controls the brightness of the image and the exponentiation is computed element-wise.

Furthermore, for the three channels of \(B\), namely \(B_{R}\), \(B_{G}\), and \(B_{B}\), and the three channels of \(D\), namely \(D_{R}\), \(D_{G}\), and \(D_{B}\), respectively, we define color distortion as follows:
\[ D_{R} = w_{R}*B_{R},\]
\[ D_{G} = w_{G}*B_{G},\]
\[ D_{B} = w_{B}*B_{B},\]
where \(w_{R}, w_{G}, w_{B}\) are small weights that distort the color space.  

We define the black level as follows: 
\[  D = max(B-bl,0), \]
where \(bl\) represents the amount of black level added to the image. Note that the closer the pixel values are to 0, the darker they are. The maximum of \(B - bl\) and 0 is taken so that no pixels are negative. \newline

Finally, we add Poison noise. Since this outputs values between \([0, 1]\), we first scale our image, apply Poison, and then re-scale as follows:
\[  D/255 = Poisson(B/255). \]


\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.3]{simulation_all}
  \caption{Simulating low-light image via our pipeline: (a)..(b)...(c)...}
\end{figure}


Our initial approach to creating simulated low light images was to randomly select values for \(\gamma, w_{R}, w_{G}, w_{B},\) and \(bl\) was to choose these randomly and validate them by visualizing the distorted image. However, this process is not good enough to yield accurate results. The purpose of the simulation step is to simulate low light images the best we can, in order to then provide a well-trained model for transfer. Therefore, we assume values for these hyper-parameters for natural low-light images belong to certain distributions. In the next step, we aim to approximate these distributions.  \newline

Let \(b_{i,j,k}\) be a pixel value from \(B\) and \(d_{i,j,k}\) be a pixel value from \(D\). We approximate \(\gamma\) finding the value such, when gamma-distortion is applied to the bright image, the average pixel value equals that to the darker image:

\[  \frac{1}{3mn}\sum_{i,j,k}b_{i,j,k}^
{1/\gamma} = \frac{1}{3mn}\sum_{i,j,k}d_{i,j,k}\] 

Similarly, for a given pair \(B\) and \(D\), we find the optimal values for \(w_{R}, w_{G}, w_{B},\) and \(bl\), such that, after applying color-distortion, and adding black-level, respectively, the average pixel value for \(B\) equals that of \(D\). \newline

We repeat this process for 200 random pairs \(B\) and \(D\) from SID and compute the distribution for each value of \(\gamma, w_{R}, w_{G}, w_{B},\) and \(bl\). These distributions are exemplified in Figure 4. Our assumption is that, by sampling these values from their respective distribution, we can create more accurate low-light images. We find these distributions by computing
the Kernel Density Estimation (KDE) and then randomly sample. 

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.4]{Distributions}
  \caption{Distributions for (a) gamma, (b) RGB weights, and (c) black level.}
  \label{fig:train2}
\end{figure}


\subsection{Model}

The intuition behind our model is that manually reviewing
a dark image at close distance would allow you to make out
details which could be typically seen in normal light 
conditions. We capture this intuition by using a Deep Convolutional Neural Network. Figure \ref{fig:model} illustrates the architecture of our CNN.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.36]{model}
  \caption{Our proposed mode..add more!!!!!!!!!!l (a) is conv and maxpool blocks, (b) is conv and upsample, (c) is conv layer, talk about sizes 32x32x3 ->.... 2x2x512... }
  \label{fig:model}
\end{figure}

Our data contains many images of different sizes. We start by taking a $64 \times 64 \times 3$ RGB image as our input. We then take a patch of this input to pass through the convolutional layers. Each co


Talk about Model Fine-Tuning: reference Figure \ref{fig:losses_all_4}

.. using the info from the composed function..
we optimize the network architecture using individual task...
 - shallow model ...
 - learning rate...
 - memory limit --> no f.c. layers, batch size
 - max pooling & upsample
 - optimize using MAE loss (proxy for PSNR)


\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.36]{losses_all_4}
  \caption{Losses... function composition... etc}
  \label{fig:losses_all_4}
\end{figure}


After we do all the fine-tuning, we run it for 100 epochs. This took approx 100 hours, on Tesla K whatever....
The results are shown in Figure \ref{fig:simulated_prediction}
prediction after epochs after a series of epochs, to show progression --> it learns to brighten quickly, still needs to do color and sharpness......

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.4]{simmulated_pred_each_epoch}
  \caption{......(a) dark, (b) after 1 epoch, (c) after 45 epochs (d) after 65 epochs, (e) after 100 epochs - final prediction, (f) is true image}
  \label{fig:simulated_prediction}
\end{figure}


\subsection{Transfer Learning}

Talk about transfer learning & how we did it

We frose all but last two layers (blue layers indicated in Figure \ref{fig:model}) and retrained on SID data ...
trained for 250 epochs (approx 7 hours)... 

\section{Experiment}

Results. We evaluate our model using the PSNR. Because PSNR is a common metric in the imaging community, 
PSNR formula is ............
bigger is better.... ..

Results Table

\begin{table}
  \caption{Results}
  \label{results_table}
  \centering
  \begin{tabular}{lllll}
    \toprule
    1 & 2l  & 3 & 4 & 5
    \midrule
    1 & 2l  & 3 & 4 & 5  \\
    \bottomrule
  \end{tabular}
\end{table}


\section{Discussion}

Currently, we are simulating low light images using (Section 3.4) using
raw images from the SID dataset. A drawback of this approach is that we
can modify the pixels of the image to simulate low light but we cannot
view these modifications as a RGB image. Our proposed next step is to modify
our model is so it is compatible with RGB images as input. We have identified
blocks within the model which most likely need to be changed. After
updating our model to accept RGB input, we will be implementing transfer
learning to obtain the final result. 

.........Discussion stuff from slides
+ path forward (what needs to be done next).
....


\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
